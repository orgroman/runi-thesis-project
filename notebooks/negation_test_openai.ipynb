{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Azure Key Vault and OpenAI Credentials\n",
    "\n",
    "Securely retrieve OpenAI API key from Azure Key Vault for authentication.\n",
    "This ensures sensitive credentials are not hardcoded in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully retrieved OpenAI API key from Azure Key Vault\n"
     ]
    }
   ],
   "source": [
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.keyvault.secrets import SecretClient\n",
    "import os\n",
    "\n",
    "def get_openai_key():\n",
    "    \"\"\"Retrieve OpenAI API key from Azure Key Vault\"\"\"\n",
    "    try:\n",
    "        # Initialize the Azure credentials\n",
    "        credential = DefaultAzureCredential()\n",
    "        \n",
    "        # Create a secret client\n",
    "        vault_url = f\"https://kvrunithesis.vault.azure.net/\"\n",
    "        secret_client = SecretClient(vault_url=vault_url, credential=credential)\n",
    "        \n",
    "        # Get the secret\n",
    "        secret = secret_client.get_secret(\"alon-thesis-openai-key\")\n",
    "        \n",
    "        # Set as environment variable\n",
    "        os.environ[\"OPENAI_API_KEY\"] = secret.value\n",
    "        \n",
    "        print(\"Successfully retrieved OpenAI API key from Azure Key Vault\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving secret from Key Vault: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Retrieve and set the OpenAI API key\n",
    "get_openai_key()\n",
    "\n",
    "# Now you can initialize the OpenAI client which will automatically use the environment variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the test data\n",
    "csv_file = r'C:\\Users\\orgrd\\workspace\\data\\patentmatch_test\\patentmatch_test_no_claims.csv'\n",
    "df = pd.read_csv(csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>index</th>\n",
       "      <th>claim_id</th>\n",
       "      <th>patent_application_id</th>\n",
       "      <th>cited_document_id</th>\n",
       "      <th>text</th>\n",
       "      <th>text_b</th>\n",
       "      <th>label</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5113165</td>\n",
       "      <td>5113165</td>\n",
       "      <td>111187_0</td>\n",
       "      <td>EP3157302A1</td>\n",
       "      <td>EP2903333</td>\n",
       "      <td>A network of handling a paging procedure in a ...</td>\n",
       "      <td>FIG.16 is a diagram illustrating an example of...</td>\n",
       "      <td>0</td>\n",
       "      <td>20170419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5658863</td>\n",
       "      <td>5658863</td>\n",
       "      <td>209068_1</td>\n",
       "      <td>EP3202314A1</td>\n",
       "      <td>EP2229880</td>\n",
       "      <td>A sensor information processing program for ca...</td>\n",
       "      <td>In a first step the fundamental movement frequ...</td>\n",
       "      <td>1</td>\n",
       "      <td>20170809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5584990</td>\n",
       "      <td>5584990</td>\n",
       "      <td>171472_0</td>\n",
       "      <td>EP3196007A1</td>\n",
       "      <td>EP2939828</td>\n",
       "      <td>A moulded trim part for a vehicle according to...</td>\n",
       "      <td>It was found that the thermoplastic polyuretha...</td>\n",
       "      <td>0</td>\n",
       "      <td>20170726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5137320</td>\n",
       "      <td>5137320</td>\n",
       "      <td>87572_0</td>\n",
       "      <td>EP3160147A1</td>\n",
       "      <td>EP1670252</td>\n",
       "      <td>A method for fast channel change characterized...</td>\n",
       "      <td>As to the issue of delivery modes the strategy...</td>\n",
       "      <td>0</td>\n",
       "      <td>20170426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5800528</td>\n",
       "      <td>5800528</td>\n",
       "      <td>204115_0</td>\n",
       "      <td>EP3217403A1</td>\n",
       "      <td>EP1855216</td>\n",
       "      <td>An audio asset information storage system comp...</td>\n",
       "      <td>Further it is assumed in the above circumstanc...</td>\n",
       "      <td>0</td>\n",
       "      <td>20170913</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0    index  claim_id patent_application_id cited_document_id  \\\n",
       "0     5113165  5113165  111187_0           EP3157302A1         EP2903333   \n",
       "1     5658863  5658863  209068_1           EP3202314A1         EP2229880   \n",
       "2     5584990  5584990  171472_0           EP3196007A1         EP2939828   \n",
       "3     5137320  5137320   87572_0           EP3160147A1         EP1670252   \n",
       "4     5800528  5800528  204115_0           EP3217403A1         EP1855216   \n",
       "\n",
       "                                                text  \\\n",
       "0  A network of handling a paging procedure in a ...   \n",
       "1  A sensor information processing program for ca...   \n",
       "2  A moulded trim part for a vehicle according to...   \n",
       "3  A method for fast channel change characterized...   \n",
       "4  An audio asset information storage system comp...   \n",
       "\n",
       "                                              text_b  label      date  \n",
       "0  FIG.16 is a diagram illustrating an example of...      0  20170419  \n",
       "1  In a first step the fundamental movement frequ...      1  20170809  \n",
       "2  It was found that the thermoplastic polyuretha...      0  20170726  \n",
       "3  As to the issue of delivery modes the strategy...      0  20170426  \n",
       "4  Further it is assumed in the above circumstanc...      0  20170913  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare JSONL Files for OpenAI Processing\n",
    "\n",
    "This section prepares the data for batch processing with OpenAI's API. Here's what we're doing:\n",
    "\n",
    "1. **Setup**: Import required libraries and configure logging\n",
    "2. **Data Model**: Define a Pydantic model `NegationResponse` to validate OpenAI's responses\n",
    "3. **Batch Processing**: \n",
    "   - Split data into batches of 1000 rows each\n",
    "   - Create JSONL files with proper OpenAI API format\n",
    "   - Each line contains:\n",
    "     - Custom ID for tracking\n",
    "     - API endpoint\n",
    "     - Request body with messages and response format\n",
    "4. **Output**: Save batches as separate JSONL files in `output_jsonl` directory\n",
    "\n",
    "The JSONL format is required for OpenAI's batch processing endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 1/373\n",
      "Processing batch 2/373\n",
      "Processing batch 3/373\n",
      "Processing batch 4/373\n",
      "Processing batch 5/373\n",
      "Processing batch 6/373\n",
      "Processing batch 7/373\n",
      "Processing batch 8/373\n",
      "Processing batch 9/373\n",
      "Processing batch 10/373\n",
      "Processing batch 11/373\n",
      "Processing batch 12/373\n",
      "Processing batch 13/373\n",
      "Processing batch 14/373\n",
      "Processing batch 15/373\n",
      "Processing batch 16/373\n",
      "Processing batch 17/373\n",
      "Processing batch 18/373\n",
      "Processing batch 19/373\n",
      "Processing batch 20/373\n",
      "Processing batch 21/373\n",
      "Processing batch 22/373\n",
      "Processing batch 23/373\n",
      "Processing batch 24/373\n",
      "Processing batch 25/373\n",
      "Processing batch 26/373\n",
      "Processing batch 27/373\n",
      "Processing batch 28/373\n",
      "Processing batch 29/373\n",
      "Processing batch 30/373\n",
      "Processing batch 31/373\n",
      "Processing batch 32/373\n",
      "Processing batch 33/373\n",
      "Processing batch 34/373\n",
      "Processing batch 35/373\n",
      "Processing batch 36/373\n",
      "Processing batch 37/373\n",
      "Processing batch 38/373\n",
      "Processing batch 39/373\n",
      "Processing batch 40/373\n",
      "Processing batch 41/373\n",
      "Processing batch 42/373\n",
      "Processing batch 43/373\n",
      "Processing batch 44/373\n",
      "Processing batch 45/373\n",
      "Processing batch 46/373\n",
      "Processing batch 47/373\n",
      "Processing batch 48/373\n",
      "Processing batch 49/373\n",
      "Processing batch 50/373\n",
      "Processing batch 51/373\n",
      "Processing batch 52/373\n",
      "Processing batch 53/373\n",
      "Processing batch 54/373\n",
      "Processing batch 55/373\n",
      "Processing batch 56/373\n",
      "Processing batch 57/373\n",
      "Processing batch 58/373\n",
      "Processing batch 59/373\n",
      "Processing batch 60/373\n",
      "Processing batch 61/373\n",
      "Processing batch 62/373\n",
      "Processing batch 63/373\n",
      "Processing batch 64/373\n",
      "Processing batch 65/373\n",
      "Processing batch 66/373\n",
      "Processing batch 67/373\n",
      "Processing batch 68/373\n",
      "Processing batch 69/373\n",
      "Processing batch 70/373\n",
      "Processing batch 71/373\n",
      "Processing batch 72/373\n",
      "Processing batch 73/373\n",
      "Processing batch 74/373\n",
      "Processing batch 75/373\n",
      "Processing batch 76/373\n",
      "Processing batch 77/373\n",
      "Processing batch 78/373\n",
      "Processing batch 79/373\n",
      "Processing batch 80/373\n",
      "Processing batch 81/373\n",
      "Processing batch 82/373\n",
      "Processing batch 83/373\n",
      "Processing batch 84/373\n",
      "Processing batch 85/373\n",
      "Processing batch 86/373\n",
      "Processing batch 87/373\n",
      "Processing batch 88/373\n",
      "Processing batch 89/373\n",
      "Processing batch 90/373\n",
      "Processing batch 91/373\n",
      "Processing batch 92/373\n",
      "Processing batch 93/373\n",
      "Processing batch 94/373\n",
      "Processing batch 95/373\n",
      "Processing batch 96/373\n",
      "Processing batch 97/373\n",
      "Processing batch 98/373\n",
      "Processing batch 99/373\n",
      "Processing batch 100/373\n",
      "Processing batch 101/373\n",
      "Processing batch 102/373\n",
      "Processing batch 103/373\n",
      "Processing batch 104/373\n",
      "Processing batch 105/373\n",
      "Processing batch 106/373\n",
      "Processing batch 107/373\n",
      "Processing batch 108/373\n",
      "Processing batch 109/373\n",
      "Processing batch 110/373\n",
      "Processing batch 111/373\n",
      "Processing batch 112/373\n",
      "Processing batch 113/373\n",
      "Processing batch 114/373\n",
      "Processing batch 115/373\n",
      "Processing batch 116/373\n",
      "Processing batch 117/373\n",
      "Processing batch 118/373\n",
      "Processing batch 119/373\n",
      "Processing batch 120/373\n",
      "Processing batch 121/373\n",
      "Processing batch 122/373\n",
      "Processing batch 123/373\n",
      "Processing batch 124/373\n",
      "Processing batch 125/373\n",
      "Processing batch 126/373\n",
      "Processing batch 127/373\n",
      "Processing batch 128/373\n",
      "Processing batch 129/373\n",
      "Processing batch 130/373\n",
      "Processing batch 131/373\n",
      "Processing batch 132/373\n",
      "Processing batch 133/373\n",
      "Processing batch 134/373\n",
      "Processing batch 135/373\n",
      "Processing batch 136/373\n",
      "Processing batch 137/373\n",
      "Processing batch 138/373\n",
      "Processing batch 139/373\n",
      "Processing batch 140/373\n",
      "Processing batch 141/373\n",
      "Processing batch 142/373\n",
      "Processing batch 143/373\n",
      "Processing batch 144/373\n",
      "Processing batch 145/373\n",
      "Processing batch 146/373\n",
      "Processing batch 147/373\n",
      "Processing batch 148/373\n",
      "Processing batch 149/373\n",
      "Processing batch 150/373\n",
      "Processing batch 151/373\n",
      "Processing batch 152/373\n",
      "Processing batch 153/373\n",
      "Processing batch 154/373\n",
      "Processing batch 155/373\n",
      "Processing batch 156/373\n",
      "Processing batch 157/373\n",
      "Processing batch 158/373\n",
      "Processing batch 159/373\n",
      "Processing batch 160/373\n",
      "Processing batch 161/373\n",
      "Processing batch 162/373\n",
      "Processing batch 163/373\n",
      "Processing batch 164/373\n",
      "Processing batch 165/373\n",
      "Processing batch 166/373\n",
      "Processing batch 167/373\n",
      "Processing batch 168/373\n",
      "Processing batch 169/373\n",
      "Processing batch 170/373\n",
      "Processing batch 171/373\n",
      "Processing batch 172/373\n",
      "Processing batch 173/373\n",
      "Processing batch 174/373\n",
      "Processing batch 175/373\n",
      "Processing batch 176/373\n",
      "Processing batch 177/373\n",
      "Processing batch 178/373\n",
      "Processing batch 179/373\n",
      "Processing batch 180/373\n",
      "Processing batch 181/373\n",
      "Processing batch 182/373\n",
      "Processing batch 183/373\n",
      "Processing batch 184/373\n",
      "Processing batch 185/373\n",
      "Processing batch 186/373\n",
      "Processing batch 187/373\n",
      "Processing batch 188/373\n",
      "Processing batch 189/373\n",
      "Processing batch 190/373\n",
      "Processing batch 191/373\n",
      "Processing batch 192/373\n",
      "Processing batch 193/373\n",
      "Processing batch 194/373\n",
      "Processing batch 195/373\n",
      "Processing batch 196/373\n",
      "Processing batch 197/373\n",
      "Processing batch 198/373\n",
      "Processing batch 199/373\n",
      "Processing batch 200/373\n",
      "Processing batch 201/373\n",
      "Processing batch 202/373\n",
      "Processing batch 203/373\n",
      "Processing batch 204/373\n",
      "Processing batch 205/373\n",
      "Processing batch 206/373\n",
      "Processing batch 207/373\n",
      "Processing batch 208/373\n",
      "Processing batch 209/373\n",
      "Processing batch 210/373\n",
      "Processing batch 211/373\n",
      "Processing batch 212/373\n",
      "Processing batch 213/373\n",
      "Processing batch 214/373\n",
      "Processing batch 215/373\n",
      "Processing batch 216/373\n",
      "Processing batch 217/373\n",
      "Processing batch 218/373\n",
      "Processing batch 219/373\n",
      "Processing batch 220/373\n",
      "Processing batch 221/373\n",
      "Processing batch 222/373\n",
      "Processing batch 223/373\n",
      "Processing batch 224/373\n",
      "Processing batch 225/373\n",
      "Processing batch 226/373\n",
      "Processing batch 227/373\n",
      "Processing batch 228/373\n",
      "Processing batch 229/373\n",
      "Processing batch 230/373\n",
      "Processing batch 231/373\n",
      "Processing batch 232/373\n",
      "Processing batch 233/373\n",
      "Processing batch 234/373\n",
      "Processing batch 235/373\n",
      "Processing batch 236/373\n",
      "Processing batch 237/373\n",
      "Processing batch 238/373\n",
      "Processing batch 239/373\n",
      "Processing batch 240/373\n",
      "Processing batch 241/373\n",
      "Processing batch 242/373\n",
      "Processing batch 243/373\n",
      "Processing batch 244/373\n",
      "Processing batch 245/373\n",
      "Processing batch 246/373\n",
      "Processing batch 247/373\n",
      "Processing batch 248/373\n",
      "Processing batch 249/373\n",
      "Processing batch 250/373\n",
      "Processing batch 251/373\n",
      "Processing batch 252/373\n",
      "Processing batch 253/373\n",
      "Processing batch 254/373\n",
      "Processing batch 255/373\n",
      "Processing batch 256/373\n",
      "Processing batch 257/373\n",
      "Processing batch 258/373\n",
      "Processing batch 259/373\n",
      "Processing batch 260/373\n",
      "Processing batch 261/373\n",
      "Processing batch 262/373\n",
      "Processing batch 263/373\n",
      "Processing batch 264/373\n",
      "Processing batch 265/373\n",
      "Processing batch 266/373\n",
      "Processing batch 267/373\n",
      "Processing batch 268/373\n",
      "Processing batch 269/373\n",
      "Processing batch 270/373\n",
      "Processing batch 271/373\n",
      "Processing batch 272/373\n",
      "Processing batch 273/373\n",
      "Processing batch 274/373\n",
      "Processing batch 275/373\n",
      "Processing batch 276/373\n",
      "Processing batch 277/373\n",
      "Processing batch 278/373\n",
      "Processing batch 279/373\n",
      "Processing batch 280/373\n",
      "Processing batch 281/373\n",
      "Processing batch 282/373\n",
      "Processing batch 283/373\n",
      "Processing batch 284/373\n",
      "Processing batch 285/373\n",
      "Processing batch 286/373\n",
      "Processing batch 287/373\n",
      "Processing batch 288/373\n",
      "Processing batch 289/373\n",
      "Processing batch 290/373\n",
      "Processing batch 291/373\n",
      "Processing batch 292/373\n",
      "Processing batch 293/373\n",
      "Processing batch 294/373\n",
      "Processing batch 295/373\n",
      "Processing batch 296/373\n",
      "Processing batch 297/373\n",
      "Processing batch 298/373\n",
      "Processing batch 299/373\n",
      "Processing batch 300/373\n",
      "Processing batch 301/373\n",
      "Processing batch 302/373\n",
      "Processing batch 303/373\n",
      "Processing batch 304/373\n",
      "Processing batch 305/373\n",
      "Processing batch 306/373\n",
      "Processing batch 307/373\n",
      "Processing batch 308/373\n",
      "Processing batch 309/373\n",
      "Processing batch 310/373\n",
      "Processing batch 311/373\n",
      "Processing batch 312/373\n",
      "Processing batch 313/373\n",
      "Processing batch 314/373\n",
      "Processing batch 315/373\n",
      "Processing batch 316/373\n",
      "Processing batch 317/373\n",
      "Processing batch 318/373\n",
      "Processing batch 319/373\n",
      "Processing batch 320/373\n",
      "Processing batch 321/373\n",
      "Processing batch 322/373\n",
      "Processing batch 323/373\n",
      "Processing batch 324/373\n",
      "Processing batch 325/373\n",
      "Processing batch 326/373\n",
      "Processing batch 327/373\n",
      "Processing batch 328/373\n",
      "Processing batch 329/373\n",
      "Processing batch 330/373\n",
      "Processing batch 331/373\n",
      "Processing batch 332/373\n",
      "Processing batch 333/373\n",
      "Processing batch 334/373\n",
      "Processing batch 335/373\n",
      "Processing batch 336/373\n",
      "Processing batch 337/373\n",
      "Processing batch 338/373\n",
      "Processing batch 339/373\n",
      "Processing batch 340/373\n",
      "Processing batch 341/373\n",
      "Processing batch 342/373\n",
      "Processing batch 343/373\n",
      "Processing batch 344/373\n",
      "Processing batch 345/373\n",
      "Processing batch 346/373\n",
      "Processing batch 347/373\n",
      "Processing batch 348/373\n",
      "Processing batch 349/373\n",
      "Processing batch 350/373\n",
      "Processing batch 351/373\n",
      "Processing batch 352/373\n",
      "Processing batch 353/373\n",
      "Processing batch 354/373\n",
      "Processing batch 355/373\n",
      "Processing batch 356/373\n",
      "Processing batch 357/373\n",
      "Processing batch 358/373\n",
      "Processing batch 359/373\n",
      "Processing batch 360/373\n",
      "Processing batch 361/373\n",
      "Processing batch 362/373\n",
      "Processing batch 363/373\n",
      "Processing batch 364/373\n",
      "Processing batch 365/373\n",
      "Processing batch 366/373\n",
      "Processing batch 367/373\n",
      "Processing batch 368/373\n",
      "Processing batch 369/373\n",
      "Processing batch 370/373\n",
      "Processing batch 371/373\n",
      "Processing batch 372/373\n",
      "Processing batch 373/373\n",
      "\n",
      "Sample output from first batch:\n",
      "{\"custom_id\": \"request_text_EP3157302A1_5113165\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"gpt-4-turbo-preview\", \"messages\": [{\"role\": \"system\", \"content\": \"Analyze the text for negations and identify their types.\"}, {\"role\": \"user\", \"content\": \"Analyze the following text: A network of handling a paging procedure in a wireless communication system the network comprising a storage unit 210 for storing instructions and a processing circuit 200 coupled to the storage unit 210 wherein the storage unit 210 stores and the processing circuit 200 is configured to execute the instructions of receiving a first Non Access Stratum NAS message comprising a first time interval for monitoring a paging occasion and a second time interval for monitoring the paging occasion from a communication device wherein the second time interval is larger than the first time interval transmitting a second NAS message comprising a third time interval for monitoring the paging occasion to the communication device and storing the first time interval and the third time interval after receiving the first NAS message wherein the third time interval is larger than the first time interval performing the paging procedure by using the third time interval when a packet data network PDN connection for an emergency bearer service is not established or not establishing and performing the paging procedure by using the first time interval when the PDN connection for the emergency bearer service is established or establishing.\"}], \"response_format\": {\"type\": \"json_schema\", \"json_schema\": {\"name\": \"negation_response\", \"schema\": {\"properties\": {\"negation_present\": {\"title\": \"Negation Present\", \"type\": \"boolean\"}, \"negation_types\": {\"anyOf\": [{\"items\": {\"type\": \"string\"}, \"type\": \"array\"}, {\"type\": \"null\"}], \"title\": \"Negation Types\"}, \"short_explanation\": {\"title\": \"Short Explanation\", \"type\": \"string\"}}, \"required\": [\"negation_present\", \"negation_types\", \"short_explanation\"], \"title\": \"NegationResponse\", \"type\": \"object\"}}}, \"max_tokens\": 500}}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prepare the openai required jsonl files\n",
    "from functools import partial\n",
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import List, Optional\n",
    "from pydantic import BaseModel\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Define the NegationResponse model\n",
    "class NegationResponse(BaseModel):\n",
    "    negation_present: bool\n",
    "    negation_types: Optional[List[str]]\n",
    "    short_explanation: str\n",
    "\n",
    "# Create output directory\n",
    "output_dir = Path('output_jsonl')\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Process in smaller batches\n",
    "batch_size = 1000\n",
    "num_batches = len(df) // batch_size + 1\n",
    "\n",
    "def create_jsonl_line(row, column):\n",
    "    text = row[column]\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"Analyze the text for negations and identify their types.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Analyze the following text: {text}\"}\n",
    "    ]\n",
    "    \n",
    "    body = {\n",
    "        \"model\": \"gpt-4-turbo-preview\",\n",
    "        \"messages\": messages,\n",
    "        \"response_format\": {\n",
    "            \"type\": \"json_schema\",\n",
    "            \"json_schema\": {\n",
    "                \"name\": \"negation_response\",\n",
    "                \"schema\": NegationResponse.model_json_schema()\n",
    "            }\n",
    "        },\n",
    "        \"max_tokens\": 500\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        \"custom_id\": f'request_{column}_{row[\"patent_application_id\"]}_{row[\"index\"]}',\n",
    "        \"method\": \"POST\",\n",
    "        \"url\": \"/v1/chat/completions\",\n",
    "        \"body\": body\n",
    "    }\n",
    "\n",
    "# Process batches\n",
    "for i in range(num_batches):\n",
    "    start_idx = i * batch_size\n",
    "    end_idx = min((i + 1) * batch_size, len(df))\n",
    "    batch_df = df.iloc[start_idx:end_idx]\n",
    "    \n",
    "    print(f\"Processing batch {i + 1}/{num_batches}\")\n",
    "    lines = batch_df.apply(create_jsonl_line, axis=1, args=('text',))\n",
    "    \n",
    "    with open(output_dir / f\"batch_{i}.jsonl\", \"w\", encoding='utf-8') as f:\n",
    "        for line in lines:\n",
    "            f.write(json.dumps(line, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "# Show sample output\n",
    "print(\"\\nSample output from first batch:\")\n",
    "with open(output_dir / \"batch_0.jsonl\", \"r\", encoding='utf-8') as f:\n",
    "    print(f.readline())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading batch_0.jsonl...\n",
      "Uploading batch_1.jsonl...\n",
      "Uploading batch_10.jsonl...\n",
      "Uploading batch_100.jsonl...\n",
      "Uploading batch_101.jsonl...\n",
      "Uploading batch_102.jsonl...\n",
      "Uploading batch_103.jsonl...\n",
      "Uploading batch_104.jsonl...\n",
      "Uploading batch_105.jsonl...\n",
      "Uploading batch_106.jsonl...\n",
      "Uploading batch_107.jsonl...\n",
      "Uploading batch_108.jsonl...\n",
      "Uploading batch_109.jsonl...\n",
      "Uploading batch_11.jsonl...\n",
      "Uploading batch_110.jsonl...\n",
      "Uploading batch_111.jsonl...\n",
      "Uploading batch_112.jsonl...\n",
      "Uploading batch_113.jsonl...\n",
      "Uploading batch_114.jsonl...\n",
      "Uploading batch_115.jsonl...\n",
      "Uploading batch_116.jsonl...\n",
      "Uploading batch_117.jsonl...\n",
      "Uploading batch_118.jsonl...\n",
      "Uploading batch_119.jsonl...\n",
      "Uploading batch_12.jsonl...\n",
      "Uploading batch_120.jsonl...\n",
      "Uploading batch_121.jsonl...\n",
      "Uploading batch_122.jsonl...\n",
      "Uploading batch_123.jsonl...\n",
      "Uploading batch_124.jsonl...\n",
      "Uploading batch_125.jsonl...\n",
      "Uploading batch_126.jsonl...\n",
      "Uploading batch_127.jsonl...\n",
      "Uploading batch_128.jsonl...\n",
      "Uploading batch_129.jsonl...\n",
      "Uploading batch_13.jsonl...\n",
      "Uploading batch_130.jsonl...\n",
      "Uploading batch_131.jsonl...\n",
      "Uploading batch_132.jsonl...\n",
      "Uploading batch_133.jsonl...\n",
      "Uploading batch_134.jsonl...\n",
      "Uploading batch_135.jsonl...\n",
      "Uploading batch_136.jsonl...\n",
      "Uploading batch_137.jsonl...\n",
      "Uploading batch_138.jsonl...\n",
      "Uploading batch_139.jsonl...\n",
      "Uploading batch_14.jsonl...\n",
      "Uploading batch_140.jsonl...\n",
      "Uploading batch_141.jsonl...\n",
      "Uploading batch_142.jsonl...\n",
      "Uploading batch_143.jsonl...\n",
      "Uploading batch_144.jsonl...\n",
      "Uploading batch_145.jsonl...\n",
      "Uploading batch_146.jsonl...\n",
      "Uploading batch_147.jsonl...\n",
      "Uploading batch_148.jsonl...\n",
      "Uploading batch_149.jsonl...\n",
      "Uploading batch_15.jsonl...\n",
      "Uploading batch_150.jsonl...\n",
      "Uploading batch_151.jsonl...\n",
      "Uploading batch_152.jsonl...\n",
      "Uploading batch_153.jsonl...\n",
      "Uploading batch_154.jsonl...\n",
      "Uploading batch_155.jsonl...\n",
      "Uploading batch_156.jsonl...\n",
      "Uploading batch_157.jsonl...\n",
      "Uploading batch_158.jsonl...\n",
      "Uploading batch_159.jsonl...\n",
      "Uploading batch_16.jsonl...\n",
      "Uploading batch_160.jsonl...\n",
      "Uploading batch_161.jsonl...\n",
      "Uploading batch_162.jsonl...\n",
      "Uploading batch_163.jsonl...\n",
      "Uploading batch_164.jsonl...\n",
      "Uploading batch_165.jsonl...\n",
      "Uploading batch_166.jsonl...\n",
      "Uploading batch_167.jsonl...\n",
      "Uploading batch_168.jsonl...\n",
      "Uploading batch_169.jsonl...\n",
      "Uploading batch_17.jsonl...\n",
      "Uploading batch_170.jsonl...\n",
      "Uploading batch_171.jsonl...\n",
      "Uploading batch_172.jsonl...\n",
      "Uploading batch_173.jsonl...\n",
      "Uploading batch_174.jsonl...\n",
      "Uploading batch_175.jsonl...\n",
      "Uploading batch_176.jsonl...\n",
      "Uploading batch_177.jsonl...\n",
      "Uploading batch_178.jsonl...\n",
      "Uploading batch_179.jsonl...\n",
      "Uploading batch_18.jsonl...\n",
      "Uploading batch_180.jsonl...\n",
      "Uploading batch_181.jsonl...\n",
      "Uploading batch_182.jsonl...\n",
      "Uploading batch_183.jsonl...\n",
      "Uploading batch_184.jsonl...\n",
      "Uploading batch_185.jsonl...\n",
      "Uploading batch_186.jsonl...\n",
      "Uploading batch_187.jsonl...\n",
      "Uploading batch_188.jsonl...\n",
      "Uploading batch_189.jsonl...\n",
      "Uploading batch_19.jsonl...\n",
      "Uploading batch_190.jsonl...\n",
      "Uploading batch_191.jsonl...\n",
      "Uploading batch_192.jsonl...\n",
      "Uploading batch_193.jsonl...\n",
      "Uploading batch_194.jsonl...\n",
      "Uploading batch_195.jsonl...\n",
      "Uploading batch_196.jsonl...\n",
      "Uploading batch_197.jsonl...\n",
      "Uploading batch_198.jsonl...\n",
      "Uploading batch_199.jsonl...\n",
      "Uploading batch_2.jsonl...\n",
      "Uploading batch_20.jsonl...\n",
      "Uploading batch_200.jsonl...\n",
      "Uploading batch_201.jsonl...\n",
      "Uploading batch_202.jsonl...\n",
      "Uploading batch_203.jsonl...\n",
      "Uploading batch_204.jsonl...\n",
      "Uploading batch_205.jsonl...\n",
      "Uploading batch_206.jsonl...\n",
      "Uploading batch_207.jsonl...\n",
      "Uploading batch_208.jsonl...\n",
      "Uploading batch_209.jsonl...\n",
      "Uploading batch_21.jsonl...\n",
      "Uploading batch_210.jsonl...\n",
      "Uploading batch_211.jsonl...\n",
      "Uploading batch_212.jsonl...\n",
      "Uploading batch_213.jsonl...\n",
      "Uploading batch_214.jsonl...\n",
      "Uploading batch_215.jsonl...\n",
      "Uploading batch_216.jsonl...\n",
      "Uploading batch_217.jsonl...\n",
      "Uploading batch_218.jsonl...\n",
      "Uploading batch_219.jsonl...\n",
      "Uploading batch_22.jsonl...\n",
      "Uploading batch_220.jsonl...\n",
      "Uploading batch_221.jsonl...\n",
      "Uploading batch_222.jsonl...\n",
      "Uploading batch_223.jsonl...\n",
      "Uploading batch_224.jsonl...\n",
      "Uploading batch_225.jsonl...\n",
      "Uploading batch_226.jsonl...\n",
      "Uploading batch_227.jsonl...\n",
      "Uploading batch_228.jsonl...\n",
      "Uploading batch_229.jsonl...\n",
      "Uploading batch_23.jsonl...\n",
      "Uploading batch_230.jsonl...\n",
      "Uploading batch_231.jsonl...\n",
      "Uploading batch_232.jsonl...\n",
      "Uploading batch_233.jsonl...\n",
      "Uploading batch_234.jsonl...\n",
      "Uploading batch_235.jsonl...\n",
      "Uploading batch_236.jsonl...\n",
      "Uploading batch_237.jsonl...\n",
      "Uploading batch_238.jsonl...\n",
      "Uploading batch_239.jsonl...\n",
      "Uploading batch_24.jsonl...\n",
      "Uploading batch_240.jsonl...\n",
      "Uploading batch_241.jsonl...\n",
      "Uploading batch_242.jsonl...\n",
      "Uploading batch_243.jsonl...\n",
      "Uploading batch_244.jsonl...\n",
      "Uploading batch_245.jsonl...\n",
      "Uploading batch_246.jsonl...\n",
      "Uploading batch_247.jsonl...\n",
      "Uploading batch_248.jsonl...\n",
      "Uploading batch_249.jsonl...\n",
      "Uploading batch_25.jsonl...\n",
      "Uploading batch_250.jsonl...\n",
      "Uploading batch_251.jsonl...\n",
      "Uploading batch_252.jsonl...\n",
      "Uploading batch_253.jsonl...\n",
      "Uploading batch_254.jsonl...\n",
      "Uploading batch_255.jsonl...\n",
      "Uploading batch_256.jsonl...\n",
      "Uploading batch_257.jsonl...\n",
      "Uploading batch_258.jsonl...\n",
      "Uploading batch_259.jsonl...\n",
      "Uploading batch_26.jsonl...\n",
      "Uploading batch_260.jsonl...\n",
      "Uploading batch_261.jsonl...\n",
      "Uploading batch_262.jsonl...\n",
      "Uploading batch_263.jsonl...\n",
      "Uploading batch_264.jsonl...\n",
      "Uploading batch_265.jsonl...\n",
      "Uploading batch_266.jsonl...\n",
      "Uploading batch_267.jsonl...\n",
      "Uploading batch_268.jsonl...\n",
      "Uploading batch_269.jsonl...\n",
      "Uploading batch_27.jsonl...\n",
      "Uploading batch_270.jsonl...\n",
      "Uploading batch_271.jsonl...\n",
      "Uploading batch_272.jsonl...\n",
      "Uploading batch_273.jsonl...\n",
      "Uploading batch_274.jsonl...\n",
      "Uploading batch_275.jsonl...\n",
      "Uploading batch_276.jsonl...\n",
      "Uploading batch_277.jsonl...\n",
      "Uploading batch_278.jsonl...\n",
      "Uploading batch_279.jsonl...\n",
      "Uploading batch_28.jsonl...\n",
      "Uploading batch_280.jsonl...\n",
      "Uploading batch_281.jsonl...\n",
      "Uploading batch_282.jsonl...\n",
      "Uploading batch_283.jsonl...\n",
      "Uploading batch_284.jsonl...\n",
      "Uploading batch_285.jsonl...\n",
      "Uploading batch_286.jsonl...\n",
      "Uploading batch_287.jsonl...\n",
      "Uploading batch_288.jsonl...\n",
      "Uploading batch_289.jsonl...\n",
      "Uploading batch_29.jsonl...\n",
      "Uploading batch_290.jsonl...\n",
      "Uploading batch_291.jsonl...\n",
      "Uploading batch_292.jsonl...\n",
      "Uploading batch_293.jsonl...\n",
      "Uploading batch_294.jsonl...\n",
      "Uploading batch_295.jsonl...\n",
      "Uploading batch_296.jsonl...\n",
      "Uploading batch_297.jsonl...\n",
      "Uploading batch_298.jsonl...\n",
      "Uploading batch_299.jsonl...\n",
      "Uploading batch_3.jsonl...\n",
      "Uploading batch_30.jsonl...\n",
      "Uploading batch_300.jsonl...\n",
      "Uploading batch_301.jsonl...\n",
      "Uploading batch_302.jsonl...\n",
      "Uploading batch_303.jsonl...\n",
      "Uploading batch_304.jsonl...\n",
      "Uploading batch_305.jsonl...\n",
      "Uploading batch_306.jsonl...\n",
      "Uploading batch_307.jsonl...\n",
      "Uploading batch_308.jsonl...\n",
      "Uploading batch_309.jsonl...\n",
      "Uploading batch_31.jsonl...\n",
      "Uploading batch_310.jsonl...\n",
      "Uploading batch_311.jsonl...\n",
      "Uploading batch_312.jsonl...\n",
      "Uploading batch_313.jsonl...\n",
      "Uploading batch_314.jsonl...\n",
      "Uploading batch_315.jsonl...\n",
      "Uploading batch_316.jsonl...\n",
      "Uploading batch_317.jsonl...\n",
      "Uploading batch_318.jsonl...\n",
      "Uploading batch_319.jsonl...\n",
      "Uploading batch_32.jsonl...\n",
      "Uploading batch_320.jsonl...\n",
      "Uploading batch_321.jsonl...\n",
      "Uploading batch_322.jsonl...\n",
      "Uploading batch_323.jsonl...\n",
      "Uploading batch_324.jsonl...\n",
      "Uploading batch_325.jsonl...\n",
      "Uploading batch_326.jsonl...\n",
      "Uploading batch_327.jsonl...\n",
      "Uploading batch_328.jsonl...\n",
      "Uploading batch_329.jsonl...\n",
      "Uploading batch_33.jsonl...\n",
      "Uploading batch_330.jsonl...\n",
      "Uploading batch_331.jsonl...\n",
      "Uploading batch_332.jsonl...\n",
      "Uploading batch_333.jsonl...\n",
      "Uploading batch_334.jsonl...\n",
      "Uploading batch_335.jsonl...\n",
      "Uploading batch_336.jsonl...\n",
      "Uploading batch_337.jsonl...\n",
      "Uploading batch_338.jsonl...\n",
      "Uploading batch_339.jsonl...\n",
      "Uploading batch_34.jsonl...\n",
      "Uploading batch_340.jsonl...\n",
      "Uploading batch_341.jsonl...\n",
      "Uploading batch_342.jsonl...\n",
      "Uploading batch_343.jsonl...\n",
      "Uploading batch_344.jsonl...\n",
      "Uploading batch_345.jsonl...\n",
      "Uploading batch_346.jsonl...\n",
      "Uploading batch_347.jsonl...\n",
      "Uploading batch_348.jsonl...\n",
      "Uploading batch_349.jsonl...\n",
      "Uploading batch_35.jsonl...\n",
      "Uploading batch_350.jsonl...\n",
      "Uploading batch_351.jsonl...\n",
      "Uploading batch_352.jsonl...\n",
      "Uploading batch_353.jsonl...\n",
      "Uploading batch_354.jsonl...\n",
      "Uploading batch_355.jsonl...\n",
      "Uploading batch_356.jsonl...\n",
      "Uploading batch_357.jsonl...\n",
      "Uploading batch_358.jsonl...\n",
      "Uploading batch_359.jsonl...\n",
      "Uploading batch_36.jsonl...\n",
      "Uploading batch_360.jsonl...\n",
      "Uploading batch_361.jsonl...\n",
      "Uploading batch_362.jsonl...\n",
      "Uploading batch_363.jsonl...\n",
      "Uploading batch_364.jsonl...\n",
      "Uploading batch_365.jsonl...\n",
      "Uploading batch_366.jsonl...\n",
      "Uploading batch_367.jsonl...\n",
      "Uploading batch_368.jsonl...\n",
      "Uploading batch_369.jsonl...\n",
      "Uploading batch_37.jsonl...\n",
      "Uploading batch_370.jsonl...\n",
      "Uploading batch_371.jsonl...\n",
      "Uploading batch_372.jsonl...\n",
      "Uploading batch_38.jsonl...\n",
      "Uploading batch_39.jsonl...\n",
      "Uploading batch_4.jsonl...\n",
      "Uploading batch_40.jsonl...\n",
      "Uploading batch_41.jsonl...\n",
      "Uploading batch_42.jsonl...\n",
      "Uploading batch_43.jsonl...\n",
      "Uploading batch_44.jsonl...\n",
      "Uploading batch_45.jsonl...\n",
      "Uploading batch_46.jsonl...\n",
      "Uploading batch_47.jsonl...\n",
      "Uploading batch_48.jsonl...\n",
      "Uploading batch_49.jsonl...\n",
      "Uploading batch_5.jsonl...\n",
      "Uploading batch_50.jsonl...\n",
      "Uploading batch_51.jsonl...\n",
      "Uploading batch_52.jsonl...\n",
      "Uploading batch_53.jsonl...\n",
      "Uploading batch_54.jsonl...\n",
      "Uploading batch_55.jsonl...\n",
      "Uploading batch_56.jsonl...\n",
      "Uploading batch_57.jsonl...\n",
      "Uploading batch_58.jsonl...\n",
      "Uploading batch_59.jsonl...\n",
      "Uploading batch_6.jsonl...\n",
      "Uploading batch_60.jsonl...\n",
      "Uploading batch_61.jsonl...\n",
      "Uploading batch_62.jsonl...\n",
      "Uploading batch_63.jsonl...\n",
      "Uploading batch_64.jsonl...\n",
      "Uploading batch_65.jsonl...\n",
      "Uploading batch_66.jsonl...\n",
      "Uploading batch_67.jsonl...\n",
      "Uploading batch_68.jsonl...\n",
      "Uploading batch_69.jsonl...\n",
      "Uploading batch_7.jsonl...\n",
      "Uploading batch_70.jsonl...\n",
      "Uploading batch_71.jsonl...\n",
      "Uploading batch_72.jsonl...\n",
      "Uploading batch_73.jsonl...\n",
      "Uploading batch_74.jsonl...\n",
      "Uploading batch_75.jsonl...\n",
      "Uploading batch_76.jsonl...\n",
      "Uploading batch_77.jsonl...\n",
      "Uploading batch_78.jsonl...\n",
      "Uploading batch_79.jsonl...\n",
      "Uploading batch_8.jsonl...\n",
      "Uploading batch_80.jsonl...\n",
      "Uploading batch_81.jsonl...\n",
      "Uploading batch_82.jsonl...\n",
      "Uploading batch_83.jsonl...\n",
      "Uploading batch_84.jsonl...\n",
      "Uploading batch_85.jsonl...\n",
      "Uploading batch_86.jsonl...\n",
      "Uploading batch_87.jsonl...\n",
      "Uploading batch_88.jsonl...\n",
      "Uploading batch_89.jsonl...\n",
      "Uploading batch_9.jsonl...\n",
      "Uploading batch_90.jsonl...\n",
      "Uploading batch_91.jsonl...\n",
      "Uploading batch_92.jsonl...\n",
      "Uploading batch_93.jsonl...\n",
      "Uploading batch_94.jsonl...\n",
      "Uploading batch_95.jsonl...\n",
      "Uploading batch_96.jsonl...\n",
      "Uploading batch_97.jsonl...\n",
      "Uploading batch_98.jsonl...\n",
      "Uploading batch_99.jsonl...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "ClientSession._request() got an unexpected keyword argument 'files'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 78\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;66;03m# Run the main function\u001b[39;00m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 78\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m main()\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;66;03m# loop = asyncio.get_running_loop()\u001b[39;00m\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;66;03m# loop.run_until_complete(main())\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[10], line 59\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     56\u001b[0m     tasks\u001b[38;5;241m.\u001b[39mappend(upload_and_track_file(session, batch_file))\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Brief delay to avoid rate limits\u001b[39;00m\n\u001b[1;32m---> 59\u001b[0m uploaded_files \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m*\u001b[39mtasks)\n\u001b[0;32m     61\u001b[0m \u001b[38;5;66;03m# Filter out None values in case of upload failures\u001b[39;00m\n\u001b[0;32m     62\u001b[0m uploaded_files \u001b[38;5;241m=\u001b[39m [file \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m uploaded_files \u001b[38;5;28;01mif\u001b[39;00m file]\n",
      "Cell \u001b[1;32mIn[10], line 28\u001b[0m, in \u001b[0;36mupload_and_track_file\u001b[1;34m(session, jsonl_path)\u001b[0m\n\u001b[0;32m     21\u001b[0m data \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpurpose\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124manswers\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Changed from 'fine-tune' to 'answers'\u001b[39;00m\n\u001b[0;32m     23\u001b[0m }\n\u001b[0;32m     24\u001b[0m files \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m'\u001b[39m: (jsonl_path\u001b[38;5;241m.\u001b[39mname, \u001b[38;5;28mopen\u001b[39m(jsonl_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapplication/jsonl\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     26\u001b[0m }\n\u001b[1;32m---> 28\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfiles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfiles\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m response:\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[0;32m     30\u001b[0m         response_json \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m response\u001b[38;5;241m.\u001b[39mjson()\n",
      "File \u001b[1;32mc:\\Users\\orgrd\\workspace\\repos\\runi-thesis-project\\.venv\\Lib\\site-packages\\aiohttp\\client.py:1232\u001b[0m, in \u001b[0;36mClientSession.post\u001b[1;34m(self, url, data, **kwargs)\u001b[0m\n\u001b[0;32m   1227\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1228\u001b[0m     \u001b[38;5;28mself\u001b[39m, url: StrOrURL, \u001b[38;5;241m*\u001b[39m, data: Any \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any\n\u001b[0;32m   1229\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_RequestContextManager\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1230\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Perform HTTP POST request.\"\"\"\u001b[39;00m\n\u001b[0;32m   1231\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _RequestContextManager(\n\u001b[1;32m-> 1232\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhdrs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMETH_POST\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1233\u001b[0m     )\n",
      "\u001b[1;31mTypeError\u001b[0m: ClientSession._request() got an unexpected keyword argument 'files'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import asyncio\n",
    "import aiohttp\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import openai\n",
    "\n",
    "# Set your OpenAI API key\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "if not openai.api_key:\n",
    "    raise ValueError(\"Please set the OPENAI_API_KEY environment variable.\")\n",
    "\n",
    "# Define the directory containing your JSONL batch files\n",
    "batch_files_dir = Path('path/to/your/batch_files')\n",
    "\n",
    "# Create a directory for tracking OpenAI file metadata\n",
    "tracking_dir = batch_files_dir / \"tracking\"\n",
    "tracking_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "async def upload_and_track_file(session, jsonl_path):\n",
    "    \"\"\"Upload a JSONL file to OpenAI and track its metadata asynchronously.\"\"\"\n",
    "    url = 'https://api.openai.com/v1/files'\n",
    "    headers = {\n",
    "        'Authorization': f\"Bearer {openai.api_key}\",\n",
    "    }\n",
    "    form_data = aiohttp.FormData()\n",
    "    form_data.add_field('purpose', 'batch')  # Set purpose to 'batch' for batch processing\n",
    "    form_data.add_field('file', jsonl_path.open('rb'), filename=jsonl_path.name, content_type='application/jsonl')\n",
    "\n",
    "    async with session.post(url, headers=headers, data=form_data) as response:\n",
    "        if response.status == 200:\n",
    "            response_json = await response.json()\n",
    "            tracking_info = {\n",
    "                \"file_id\": response_json['id'],\n",
    "                \"original_filename\": jsonl_path.name,\n",
    "                \"status\": response_json['status'],\n",
    "                \"created_at\": response_json['created_at'],\n",
    "                \"upload_timestamp\": datetime.now().isoformat(),\n",
    "                \"bytes\": response_json['bytes'],\n",
    "                \"purpose\": response_json['purpose']\n",
    "            }\n",
    "            # Save tracking info\n",
    "            tracking_file = tracking_dir / f\"{response_json['id']}_metadata.json\"\n",
    "            with open(tracking_file, 'w') as f:\n",
    "                json.dump(tracking_info, f, indent=2)\n",
    "            return tracking_info\n",
    "        else:\n",
    "            error_text = await response.text()\n",
    "            print(f\"Error uploading {jsonl_path.name}: {error_text}\")\n",
    "            return None\n",
    "\n",
    "async def main_2():\n",
    "    \"\"\"Main function to upload new batch files asynchronously.\"\"\"\n",
    "    print(\"\\nUploading new batch files to OpenAI...\")\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        tasks = []\n",
    "        for batch_file in sorted(batch_files_dir.glob(\"batch_*.jsonl\")):\n",
    "            print(f\"Uploading {batch_file.name}...\")\n",
    "            tasks.append(upload_and_track_file(session, batch_file))\n",
    "            await asyncio.sleep(1)  # Brief delay to avoid rate limits\n",
    "\n",
    "        uploaded_files = await asyncio.gather(*tasks)\n",
    "\n",
    "        # Filter out None values in case of upload failures\n",
    "        uploaded_files = [file for file in uploaded_files if file]\n",
    "\n",
    "        # Save summary of all uploads\n",
    "        summary_file = tracking_dir / \"upload_summary.json\"\n",
    "        with open(summary_file, 'w') as f:\n",
    "            json.dump({\n",
    "                \"upload_timestamp\": datetime.now().isoformat(),\n",
    "                \"total_files\": len(uploaded_files),\n",
    "                \"files\": uploaded_files\n",
    "            }, f, indent=2)\n",
    "\n",
    "        print(f\"\\nUploaded {len(uploaded_files)} files to OpenAI\")\n",
    "        print(f\"Tracking information saved to {tracking_dir}\")\n",
    "\n",
    "await main_2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process OpenAI Batch Requests\n",
    "\n",
    "This section handles batch processing with OpenAI, including:\n",
    "1. Reading file IDs from tracking directory\n",
    "2. Managing batch submissions (max 50 concurrent batches)\n",
    "3. Tracking progress and handling errors\n",
    "4. Retrying failed requests\n",
    "5. Saving results as they arrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import asyncio\n",
    "import aiohttp\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Define the output directory\n",
    "output_dir = Path('output_jsonl')\n",
    "\n",
    "# Create a directory for tracking OpenAI file metadata\n",
    "tracking_dir = output_dir / \"tracking\"\n",
    "tracking_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "async def upload_and_track_file(session, jsonl_path):\n",
    "    \"\"\"Upload a JSONL file to OpenAI and track its metadata asynchronously.\"\"\"\n",
    "    url = 'https://api.openai.com/v1/files'\n",
    "    headers = {\n",
    "        'Authorization': f\"Bearer {os.getenv('OPENAI_API_KEY')}\",\n",
    "    }\n",
    "    data = aiohttp.FormData()\n",
    "    data.add_field('purpose', 'fine-tune')  # 'fine-tune' is the current valid purpose\n",
    "    data.add_field('file', open(jsonl_path, 'rb'), filename=jsonl_path.name, content_type='application/jsonl')\n",
    "\n",
    "    async with session.post(url, headers=headers, data=data) as response:\n",
    "        if response.status == 200:\n",
    "            response_json = await response.json()\n",
    "            tracking_info = {\n",
    "                \"file_id\": response_json['id'],\n",
    "                \"original_filename\": jsonl_path.name,\n",
    "                \"status\": response_json['status'],\n",
    "                \"created_at\": response_json['created_at'],\n",
    "                \"upload_timestamp\": datetime.now().isoformat(),\n",
    "                \"bytes\": response_json['bytes'],\n",
    "                \"purpose\": response_json['purpose']\n",
    "            }\n",
    "            # Save tracking info\n",
    "            tracking_file = tracking_dir / f\"{response_json['id']}_metadata.json\"\n",
    "            with open(tracking_file, 'w') as f:\n",
    "                json.dump(tracking_info, f, indent=2)\n",
    "            return tracking_info\n",
    "        else:\n",
    "            error_text = await response.text()\n",
    "            print(f\"Error uploading {jsonl_path.name}: {error_text}\")\n",
    "            return None\n",
    "\n",
    "async def main_2():\n",
    "    \"\"\"Main function to upload all batch files asynchronously.\"\"\"\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        tasks = []\n",
    "        for batch_file in sorted(output_dir.glob(\"batch_*.jsonl\")):\n",
    "            print(f\"Uploading {batch_file.name}...\")\n",
    "            tasks.append(upload_and_track_file(session, batch_file))\n",
    "            await asyncio.sleep(1)  # Brief delay to avoid rate limits\n",
    "\n",
    "        uploaded_files = await asyncio.gather(*tasks)\n",
    "\n",
    "        # Filter out None values in case of upload failures\n",
    "        uploaded_files = [file for file in uploaded_files if file]\n",
    "\n",
    "        # Save summary of all uploads\n",
    "        summary_file = tracking_dir / \"upload_summary.json\"\n",
    "        with open(summary_file, 'w') as f:\n",
    "            json.dump({\n",
    "                \"upload_timestamp\": datetime.now().isoformat(),\n",
    "                \"total_files\": len(uploaded_files),\n",
    "                \"files\": uploaded_files\n",
    "            }, f, indent=2)\n",
    "\n",
    "        print(f\"\\nUploaded {len(uploaded_files)} files to OpenAI\")\n",
    "        print(f\"Tracking information saved to {tracking_dir}\")\n",
    "\n",
    "# Run the main function within the existing event loop\n",
    "await main_2()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
